papers:
1. https://arxiv.org/pdf/1809.05957
2. https://arxiv.org/pdf/1804.06872
3. https://arxiv.org/pdf/1712.05055
4. https://arxiv.org/pdf/1904.04717
5. https://arxiv.org/pdf/2305.00562
6. https://arxiv.org/pdf/1812.05214
7. https://openaccess.thecvf.com/content_ICCV_2019/papers/Han_Deep_Self-Learning_From_Noisy_Labels_ICCV_2019_paper.pdf
8. https://openaccess.thecvf.com/content/WACV2022/papers/Zheltonozhskii_Contrast_To_Divide_Self-Supervised_Pre-Training_for_Learning_With_Noisy_Labels_WACV_2022_paper.pdf
9. https://arxiv.org/pdf/2306.01721

concepts:
1. Evaluation metrics for 
    1.1 Diffusion models
    1.2 Classification
    1.3 Regression
    1.4 Unsupervised learning
    1.5 Class imbalanced (focal loss, explicit class weighting, re-sampling, synthetic data)
    1.6 Overfitting and underfitting and tradeoffs for various architectures (diffusion, unsupervised, supervised-classification)

2. Optimization choice
    2.1 optimizer variants and when to use each
    2.2 scheduler variants and when to use each 
    2.3 regularization techniques
    2.4 Custom grad fn

3. Data
    3.1 Dataloader params
    3.2 IterableDataset
    3.3 Transforms 

4. Performance 
    4.1 Complexity of attention
    4.2 Complexity of diffusion
    4.3 Complexity improvements
    4.4 Additional challenges when the sequences or resolution is larger
    
